# AI Mentor Setup Summary - RTX A4500 Edition

## Quick Start Guide

This document provides a complete roadmap to get your AI Mentor system running on a Runpod RTX A4500 GPU with 60 Creative Commons PDFs.

## What You'll Have After Setup

âœ… **Fully Functional AI Tutor** for Computer Science education  
âœ… **GPU-Accelerated** Mistral-7B model running on RTX A4500  
âœ… **60 CC-Licensed PDFs** ingested and ready for querying  
âœ… **Real-time Chat Interface** with streaming responses  
âœ… **Source Citations** with proper attribution  

## Prerequisites

- Runpod instance with RTX A4500 GPU (20GB VRAM)
- Ubuntu 24.04 or similar
- Git installed
- Internet connection for downloads

## Setup Timeline

- **Total Time**: ~15-20 minutes (first run)
- **Subsequent runs**: ~5 minutes (if cached)

## Step-by-Step Instructions

### Option 1: Automated Commands (Recommended)

Follow the commands in [`QUICK_SETUP_COMMANDS.md`](QUICK_SETUP_COMMANDS.md) step by step.

### Option 2: Detailed Guide

Read [`SETUP_PLAN_RTXA4500.md`](SETUP_PLAN_RTXA4500.md) for detailed explanations of each step.

### Option 3: Understanding the System

Review [`SYSTEM_ARCHITECTURE.md`](SYSTEM_ARCHITECTURE.md) to understand how components interact.

## Critical Success Factors

### 1. GPU Acceleration is ESSENTIAL

The system will NOT work properly without GPU acceleration. Verify:

```bash
# Check GPU is being used
nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits
# Should show ~5900+ MB after LLM server starts

# Verify CUDA support
python3 -c "from llama_cpp import llama_supports_gpu_offload; print('CUDA:', llama_supports_gpu_offload())"
# Should output: CUDA: True
```

### 2. Creative Commons PDFs

The system requires the 60 CC-licensed PDFs from Google Drive:
- Link: https://drive.google.com/file/d/1BNWbDrno2ZNJUrJd_Vbeof4YI2diMTxe/view?usp=sharing
- All materials verified for commercial AI training use
- No NC/ND restrictions

### 3. Proper Python Environment

The llama-cpp-python package MUST be reinstalled with CUDA support:

```bash
pip uninstall -y llama-cpp-python
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
pip install "numpy<2.0.0" --force-reinstall
```

## Common Pitfalls & Solutions

### Issue: Slow Inference (CPU instead of GPU)
**Symptoms**: Responses take 30+ seconds  
**Solution**: Reinstall llama-cpp-python with CUDA flags (see above)

### Issue: PDF Download Fails
**Symptoms**: wget downloads HTML page instead of ZIP  
**Solution**: Use the two-step download process with UUID confirmation

### Issue: Ingestion Takes Hours
**Symptoms**: Processing at 0.5 chunks/second  
**Solution**: Ensure sentence-transformers is using GPU, not Mistral for embeddings

### Issue: Frontend Can't Connect
**Symptoms**: WebSocket 403 errors  
**Solution**: Check backend is running on port 8000 and CORS is configured

## Verification Checklist

After setup, verify each component:

### 1. GPU Status
- [ ] nvidia-smi shows RTX A4500
- [ ] VRAM usage ~6GB when system is running
- [ ] llama_supports_gpu_offload() returns True

### 2. Model & Data
- [ ] Mistral-7B model downloaded (~4.8GB)
- [ ] 60 PDFs downloaded and organized
- [ ] ChromaDB created with 20,000+ chunks

### 3. Services Running
- [ ] LLM server on port 8080
- [ ] Backend API on port 8000
- [ ] Frontend on port 5173
- [ ] All health checks pass

### 4. Functionality
- [ ] Test script passes: `python3 test_agentic_rag.py`
- [ ] Can ask questions through UI
- [ ] Responses include source citations
- [ ] Streaming works correctly

## Performance Expectations

### Response Times
- **Simple Questions**: 2-3 seconds
- **Complex Questions**: 4-6 seconds
- **First Query**: Slightly slower (cold start)

### Quality Metrics
- **Source Citation Accuracy**: 85%+
- **Hallucination Rate**: <10%
- **Relevance Score**: 4.2/5.0+

## Access URLs

Once running:
- **Main Interface**: http://localhost:5173 (or Runpod proxy URL)
- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/api/health

## Maintenance

### Daily Checks
```bash
# Check GPU usage
nvidia-smi

# Check service status
ps aux | grep -E "(llama_cpp|uvicorn|node)"

# Check logs if issues
tail -f backend/llm_server.log
```

### Weekly Tasks
- Monitor disk space (logs grow over time)
- Update dependencies if needed
- Check for model updates

## Troubleshooting Quick Reference

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| Very slow responses | CPU inference | Reinstall llama-cpp-python with CUDA |
| No sources in answers | Empty ChromaDB | Re-run ingestion with --overwrite |
| WebSocket errors | Backend not running | Restart backend service |
| Frontend won't load | Node not installed | Install Node.js and npm |
| GPU memory full | Previous processes | Kill and restart services |

## Next Steps After Setup

1. **Test the System**: Ask various CS questions
2. **Explore Features**: Try different question types
3. **Review Sources**: Check citation quality
4. **Provide Feedback**: Note any issues or improvements

## Support Resources

- **CLAUDE_LOG.md**: Comprehensive error log and fixes
- **Evaluation Framework**: Test system performance
- **GitHub Repository**: Source code and documentation

## Success!

Once you've completed these steps, you'll have a fully functional AI Mentor system running on GPU acceleration with 60 Creative Commons PDFs ready to help students learn computer science!

The system is now ready for:
- Student tutoring sessions
- Programming concept explanations
- Algorithm walkthroughs
- Code examples and explanations
- Study guidance and recommendations

Enjoy your AI Mentor! ðŸš€