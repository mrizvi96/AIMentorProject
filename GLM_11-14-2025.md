# AI Mentor Runpod Deployment Session
**Date**: November 14, 2025
**GPU**: RTX 4500 (20GB VRAM)
**Instance**: Fresh Runpod deployment

## Session Objectives
- Deploy AI mentor on fresh Runpod instance
- Restore 60+ Creative Commons PDF textbooks for knowledge base
- Fix GitHub authentication and Matrix-themed UI
- Resolve chat functionality and performance issues

## Issues Encountered & Fixes Applied

### 1. Missing PDF Collection (Expected 60, Found 8)
**Problem**: Only 8 PDFs present instead of expected 60 textbooks
**Root Cause**: Old PDF collection link with limited materials
**Fix Applied**:
- Downloaded complete collection from Google Drive: https://drive.google.com/file/d/1BNWbDrno2ZNJUrJd_Vbeof4YI2diMTxe/view?usp=sharing
- Extracted 67 Creative Commons PDFs covering comprehensive CS curriculum
- Successfully ingested all 67 PDFs into ChromaDB vector store

### 2. GitHub Authentication Missing
**Problem**: No GitHub login button, red logout button not functional, HTTP 500 errors
**Root Cause**: AuthButton component and auth.ts service missing from repository
**Fix Applied**:
- Created `frontend/src/lib/components/AuthButton.svelte` with Matrix styling
- Created `frontend/src/lib/auth.ts` with proper GitHub OAuth integration
- Fixed API response parsing to handle `{ "user": null }` structure correctly
- Restored GitHub authentication button functionality

### 3. Critical LLM Performance Issue (CPU vs GPU)
**Problem**: LLM responses taking 32+ seconds, poor performance
**Root Cause**: Model running entirely on CPU despite `--n_gpu_layers -1` flag
**Diagnosis**:
- GPU memory usage: only 333MB (should be 5GB+)
- Model layers: all assigned to CPU in llama.cpp logs
- Response time: 32+ seconds for simple queries

**Fix Applied**:
```bash
# Uninstalled existing llama-cpp-python
pip uninstall llama-cpp-python -y

# Reinstalled with forced CUDA compilation
export CMAKE_ARGS="-DGGML_CUDA=on"
export FORCE_CMAKE=1
pip install --no-cache-dir llama-cpp-python --verbose
```

**Result**:
- âœ… GPU memory usage: 6GB (proper model loading)
- âœ… Response time: ~8 seconds (4x improvement)
- âœ… CUDA backend successfully compiled and enabled

### 4. Frontend Service Interruption
**Problem**: Frontend server stopped responding on localhost:5173
**Root Cause**: tmux session got killed during development
**Fix Applied**:
```bash
tmux kill-session -t frontend
tmux new-session -d -s frontend "cd /root/AIMentorProject/frontend && npm run dev -- --host 0.0.0.0 --port 5173"
```

## Technical Specifications

### Environment Configuration
- **Container**: runpod/pytorch:1.0.2-cu1281-torch280-ubuntu2404
- **CUDA**: 12.8.1
- **PyTorch**: 2.8.0
- **OS**: Ubuntu 24.04

### LLM Configuration
- **Model**: Mistral-7B-Instruct-v0.2.Q5_K_M.gguf (5.1GB)
- **Quantization**: Q5_K_M (balanced quality/size)
- **GPU Acceleration**: All layers offloaded to RTX 4500
- **Context Window**: 4096 tokens

### Vector Database
- **Technology**: ChromaDB (file-based, no Docker required)
- **Collection**: 67 Creative Commons PDFs
- **Embeddings**: sentence-transformers with GPU acceleration
- **Storage**: ./backend/chroma_db/

### Authentication System
- **Method**: GitHub OAuth
- **Frontend**: SvelteKit with Matrix terminal theme
- **Backend**: FastAPI with session management
- **UI Components**: AuthButton with avatar display

## Performance Benchmarks

### Before GPU Fix
- **Response Time**: 32+ seconds
- **GPU Usage**: 333MB (ineffective)
- **Model Loading**: CPU only
- **User Experience**: Poor, timeouts

### After GPU Fix
- **Response Time**: ~8 seconds (75% improvement)
- **GPU Usage**: 6GB (proper utilization)
- **Model Loading**: Full GPU acceleration
- **User Experience**: Excellent, responsive

### Sample Response Quality
```json
{
  "question": "What is Python?",
  "answer": "Python is a high-level, interpreted programming language known for its simplicity and readability...",
  "sources": [
    {
      "text": "Python is one of those rare languages which can claim to be both simple and powerful...",
      "metadata": {
        "file_name": "Swaroop_C_H_byte_of_python.pdf",
        "page_label": "15"
      }
    }
  ]
}
```

## Current System Status

### âœ… All Services Operational
- **Frontend**: http://localhost:5173/ (SvelteKit dev server)
- **Backend API**: http://localhost:8000/ (FastAPI)
- **LLM Server**: http://localhost:8080/ (llama.cpp)
- **ChromaDB**: ./backend/chroma_db/ (67 PDFs ingested)

### âœ… Features Restored
- Matrix-themed terminal UI with green styling
- GitHub OAuth authentication button
- Avatar display for authenticated users
- Responsive chat interface
- GPU-accelerated LLM inference
- Source-cited RAG responses

### âœ… Performance Optimized
- GPU utilization: 6GB VRAM
- Response latency: ~8 seconds
- Document retrieval: ChromaDB vector search
- Model inference: CUDA-accelerated

## Access URLs (Runpod)
Replace `[POD-ID]` with actual Runpod pod ID:
- **Frontend**: https://[POD-ID]-5173.runpod.io
- **Backend API**: https://[POD-ID]-8000.runpod.io
- **API Documentation**: https://[POD-ID]-8000.runpod.io/docs
- **LLM Server**: https://[POD-ID]-8080.runpod.io

## Key Architectural Decisions Validated

### 1. ChromaDB over Milvus
- âœ… File-based, no Docker dependency
- âœ… Faster setup and deployment
- âœ… Suitable for educational content scale

### 2. Decoupled LLM Server
- âœ… Independent scaling and updates
- âœ… Clean separation of concerns
- âœ… Easy backend switching capability

### 3. GPU Acceleration Critical
- âœ… Essential for usable response times
- âœ… RTX 4500 handles 7B model comfortably
- âœ… CUDA compilation requirements documented

### 4. Matrix Theme & GitHub Auth
- âœ… Strong user identity and branding
- âœ… GitHub OAuth provides seamless access
- âœ… Terminal aesthetics match CS education context

## Lessons Learned

### 1. GPU Compilation Requirements
llama-cpp-python requires explicit CUDA compilation flags:
```bash
export CMAKE_ARGS="-DGGML_CUDA=on"
export FORCE_CMAKE=1
```

### 2. Authentication Component Management
Frontend components must be properly versioned and included:
- AuthButton.svelte for UI
- auth.ts for service logic
- Proper API response parsing

### 3. PDF Collection Management
Complete educational resource collections must be:
- Stored in reliable cloud storage
- Verified before deployment
- Ingestion pipeline tested end-to-end

### 4. Service Monitoring
Critical health checks:
- GPU memory usage (should be 5GB+ for 7B model)
- Response time benchmarks
- Service availability across all ports

## Future Improvements

### 1. Enhanced Performance
- Consider model quantization optimization
- Implement response caching
- Add request batching capabilities

### 2. Better Error Handling
- Graceful GPU fallback mechanisms
- Improved timeout handling
- Service health dashboards

### 3. Deployment Automation
- Scripted health checks
- Automated service recovery
- Configuration management

## Summary
Successfully restored AI Mentor functionality on fresh Runpod instance with:
- âœ… Complete 67 PDF knowledge base
- âœ… GPU-accelerated LLM inference (6GB VRAM usage)
- âœ… GitHub OAuth authentication
- âœ… Matrix-themed UI
- âœ… Responsive 8-second response times
- âœ… Full system reliability

The deployment demonstrates robust architecture with proper separation of concerns and effective GPU utilization strategies.

---

# ðŸš€ Complete GitHub Restoration Guide

## Scenario
This Runpod instance will be deleted. Future deployment requires complete restoration from GitHub using this guide.

## Prerequisites
- New Runpod instance with RTX 4500 (20GB VRAM) or similar
- Container: `runpod/pytorch:1.0.2-cu1281-torch280-ubuntu2404`
- GitHub repository access
- Internet connectivity for downloads

## Step-by-Step Restoration Process

### Step 1: Clone Repository and Setup Environment
```bash
# Navigate to workspace
cd /workspace

# Clone repository
git clone https://github.com/YOUR_USERNAME/AIMentorProject.git
cd AIMentorProject

# Check current state
git status
git log --oneline -5
```

### Step 2: Download Mistral Model (5.1GB)
```bash
# Create model directory
mkdir -p /workspace/models

# Download Mistral-7B-Instruct Q5_K_M quantized model
cd /workspace/models
wget --progress=bar:force:noscroll \
  "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf" \
  -O mistral-7b-instruct-v0.2.Q5_K_M.gguf

# Verify download completion (should be ~5.1GB)
ls -lh mistral-7b-instruct-v0.2.Q5_K_M.gguf
```

### Step 3: Setup Backend Python Environment
```bash
# Navigate to backend
cd /workspace/AIMentorProject/backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Upgrade pip
pip install -q --upgrade pip setuptools wheel

# Install dependencies
pip install -q -r requirements.txt

# Critical: Install llama-cpp-python with CUDA support
export CMAKE_ARGS="-DGGML_CUDA=on"
export FORCE_CMAKE=1
pip install --no-cache-dir llama-cpp-python --verbose
```

### Step 4: Download PDF Course Materials
```bash
# Navigate to course materials directory
mkdir -p /workspace/AIMentorProject/course_materials
cd /workspace/AIMentorProject/course_materials

# Download complete PDF collection (67 Creative Commons textbooks)
wget --no-check-certificate "https://drive.google.com/uc?export=download&id=1BNWbDrno2ZNJUrJd_Vbeof4YI2diMTxe" \
  -O cs_textbooks.zip

# Extract PDFs
unzip cs_textbooks.zip

# Verify PDF count (should be 67 files)
ls -1 *.pdf | wc -l
```

### Step 5: Ingest PDFs into ChromaDB
```bash
# Navigate to backend
cd /workspace/AIMentorProject/backend

# Activate virtual environment
source venv/bin/activate

# Ingest all PDFs (takes ~10-15 minutes)
python ingest.py --directory ../course_materials/

# Verify ChromaDB creation
ls -la chroma_db/
```

### Step 6: Start All Services

#### 6.1 Start LLM Inference Server (port 8080)
```bash
# Create tmux session for LLM server
tmux new-session -d -s llm \
  "cd /workspace/AIMentorProject && \
   source backend/venv/bin/activate && \
   python3 -m llama_cpp.server \
     --model /workspace/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf \
     --n_gpu_layers -1 \
     --n_ctx 4096 \
     --host 0.0.0.0 \
     --port 8080 \
     --chat_format mistral-instruct"

# Wait for model to load (30 seconds)
sleep 30

# Test LLM server
curl -s http://localhost:8080/v1/models | jq .
```

#### 6.2 Start Backend API Server (port 8000)
```bash
# Create tmux session for backend API
tmux new-session -d -s backend \
  "cd /workspace/AIMentorProject/backend && \
   source venv/bin/activate && \
   uvicorn main:app --host 0.0.0.0 --port 8000 --reload"

# Test backend health
curl -s http://localhost:8000/ | jq .
```

#### 6.3 Start Frontend Dev Server (port 5173)
```bash
# Navigate to frontend
cd /workspace/AIMentorProject/frontend

# Install dependencies
npm install

# Create tmux session for frontend
tmux new-session -d -s frontend \
  "npm run dev -- --host 0.0.0.0 --port 5173"

# Test frontend accessibility
sleep 5
curl -s http://localhost:5173/ | head -5
```

### Step 7: Verify System Health
```bash
# Check all tmux sessions
tmux ls

# Test GPU usage (should show 5GB+ memory)
nvidia-smi --query-gpu=memory.used,memory.total --format=csv,noheader,nounits

# Test complete chat functionality
timeout 30 curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is Python?", "conversation_id": "test"}' | jq .
```

### Step 8: Final Verification Tests

#### 8.1 GPU Performance Test
```bash
# Expected response time: ~8 seconds
time curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "Explain binary search", "conversation_id": "gpu-test"}' \
  | jq '.answer' | head -c 100
```

#### 8.2 Authentication Test
```bash
# Test GitHub auth endpoint
curl -s http://localhost:8000/api/auth/me | jq .

# Should return: {"user": null} when not authenticated
```

#### 8.3 ChromaDB Source Verification
```bash
# Test that sources are properly cited
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is machine learning?", "conversation_id": "source-test"}' \
  | jq '.sources | length'
```

## Expected Results After Restoration

### Service Health Checks
```bash
âœ… LLM Server:    http://localhost:8080/v1/models
âœ… Backend API:   http://localhost:8000/
âœ… Frontend:      http://localhost:5173/
âœ… GPU Memory:    6GB+ usage
âœ… Response Time: ~8 seconds
âœ… PDF Count:     67 documents ingested
```

### tmux Sessions Running
```bash
backend: 1 windows (FastAPI server)
frontend: 1 windows (SvelteKit dev server)
llm: 1 windows (llama.cpp inference server)
```

### Access URLs (Replace [POD-ID])
- **Frontend**: https://[POD-ID]-5173.runpod.io
- **Backend**: https://[POD-ID]-8000.runpod.io
- **API Docs**: https://[POD-ID]-8000.runpod.io/docs
- **LLM Server**: https://[POD-ID]-8080.runpod.io

## Troubleshooting Common Issues

### Issue 1: LLM Server Using CPU Instead of GPU
**Symptoms**: Slow responses (>30 seconds), low GPU memory usage
**Solution**:
```bash
# Recompile llama-cpp-python with CUDA
source backend/venv/bin/activate
pip uninstall llama-cpp-python -y
export CMAKE_ARGS="-DGGML_CUDA=on"
export FORCE_CMAKE=1
pip install --no-cache-dir llama-cpp-python --verbose

# Restart LLM server
tmux kill-session -t llm
# Then restart with Step 6.1
```

### Issue 2: Frontend Not Loading
**Symptoms**: 502/504 errors on port 5173
**Solution**:
```bash
# Check npm dependencies
cd /workspace/AIMentorProject/frontend
npm install

# Restart frontend tmux session
tmux kill-session -t frontend
tmux new-session -d -s frontend "npm run dev -- --host 0.0.0.0 --port 5173"
```

### Issue 3: Authentication Button Missing
**Symptoms**: No GitHub login button, authentication errors
**Solution**:
```bash
# Verify auth files exist
ls -la /workspace/AIMentorProject/frontend/src/lib/auth.ts
ls -la /workspace/AIMentorProject/frontend/src/lib/components/AuthButton.svelte

# If missing, they weren't committed to GitHub
# Manually recreate them from this documentation
```

### Issue 4: ChromaDB Empty
**Symptoms**: No sources in responses, empty knowledge base
**Solution**:
```bash
# Re-ingest PDFs
cd /workspace/AIMentorProject/backend
source venv/bin/activate
python ingest.py --directory ../course_materials/

# Verify ingestion
ls -la chroma_db/chroma-collection.sqlite
```

## Critical Files Committed to GitHub

### Backend Configuration
- `main.py` - FastAPI application entry point
- `requirements.txt` - Python dependencies
- `ingest.py` - PDF ingestion pipeline
- `app/services/agent_graph.py` - LangGraph RAG workflow

### Frontend Components
- `src/routes/+page.svelte` - Main chat interface with Matrix theme
- `src/lib/auth.ts` - GitHub OAuth authentication service
- `src/lib/components/AuthButton.svelte` - Login/logout component
- `src/lib/components/MessageList.svelte` - Chat message display
- `src/lib/components/ChatInput.svelte` - User input interface
- `src/lib/api.ts` - WebSocket chat service

### Deployment Scripts
- `runpod_simple_startup.sh` - Automated deployment script
- `CLAUDE.md` - Project documentation
- `GLM_11-14-2025.md` - This restoration guide

## What's NOT in GitHub (Must be Re-created)

### Large Files (>100MB)
- `/workspace/models/mistral-7b-instruct-v0.2.Q5_K_M.gguf` (5.1GB)
- `backend/chroma_db/` (Vector database, ~200MB+)
- `course_materials/*.pdf` (67 PDF files, ~500MB+)

### Generated Data
- Python virtual environment (`backend/venv/`)
- Node modules (`frontend/node_modules/`)
- Runtime logs and temporary files

## Restoration Time Estimates

| Step | Time Estimate | Notes |
|------|---------------|-------|
| Clone Repository | 1 minute | Fast |
| Download Model | 15-20 minutes | 5.1GB download |
| Setup Backend | 5-10 minutes | Dependencies + CUDA compile |
| Download PDFs | 10-15 minutes | 67 files |
| Ingest PDFs | 10-15 minutes | ChromaDB creation |
| Start Services | 2 minutes | All three services |
| Verification | 2 minutes | Health checks |
| **Total** | **45-65 minutes** | First-time setup |

**Subsequent Restorations**: 15-20 minutes (if model and PDFs are cached/backed up)

## Alternative: Pre-built Docker Images (Future Enhancement)

For faster deployments, consider building Docker images with:
- Pre-installed dependencies
- Pre-compiled llama-cpp-python with CUDA
- ChromaDB with ingested PDFs

This could reduce restoration time to 5-10 minutes.

## Success Criteria

Restoration is successful when:
- âœ… All three services running (llm, backend, frontend)
- âœ… GPU shows 6GB+ memory usage
- âœ… Chat responses complete in ~8 seconds
- âœ… Sources are properly cited from PDFs
- âœ… GitHub authentication button appears
- âœ… Matrix theme styling is applied
- âœ… No CPU-only performance issues

## Emergency Contacts & Resources

- **GitHub Repository**: https://github.com/YOUR_USERNAME/AIMentorProject
- **Model Source**: https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
- **PDF Collection**: Google Drive link in this documentation
- **Runpod Support**: https://runpod.io/support

This guide ensures complete restoration of AI Mentor functionality on any fresh Runpod instance with consistent performance and all original features intact.