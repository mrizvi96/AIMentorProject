# AI Mentor - Intelligent Computer Science Tutor

An agentic RAG (Retrieval-Augmented Generation) system designed to provide intelligent tutoring for computer science students. Built with FastAPI, SvelteKit, and powered by local LLM inference.

## ğŸš€ Quick Start (Runpod)

Every new Runpod instance, run:

```bash
# 1. Clone repository
git clone https://github.com/mrizvi96/AIMentorProject.git
cd AIMentorProject

# 2. Download Mistral model (one time, ~5 minutes)
mkdir -p /workspace/models
cd /workspace/models
wget -O Mistral-7B-Instruct-v0.2.Q5_K_M.gguf \
  "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf"
cd ~/AIMentorProject

# 3. Start all services
./START_SERVICES.sh

# 4. In separate terminals, start the servers:
# Terminal 1: LLM Server
./start_llm_server.sh

# Terminal 2: Backend API
cd backend && source venv/bin/activate
uvicorn main:app --host 0.0.0.0 --port 8000 --reload

# Terminal 3: Frontend
cd frontend
npm run dev -- --host 0.0.0.0
```

**Access the application:**
- Frontend UI: http://localhost:5173
- Backend API: http://localhost:8000/docs
- LLM Server: http://localhost:8080/v1

## ğŸ“‹ Prerequisites

- **Runpod Instance:** RTX A5000 (24GB VRAM) recommended
- **Base Image:** `runpod/pytorch:1.0.2-cu1281-torch280-ubuntu2404`
- **Python:** 3.10+
- **Node.js:** 20+
- **Docker:** For Milvus vector database

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend (SvelteKit)                          â”‚
â”‚  - Modern chat interface                       â”‚
â”‚  - Real-time message streaming                 â”‚
â”‚  - Source document viewer                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚ HTTP/WebSocket
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend (FastAPI)                             â”‚
â”‚  - RAG orchestration                           â”‚
â”‚  - Chat API endpoints                          â”‚
â”‚  - Document ingestion pipeline                 â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                          â”‚
       â”‚ Query/Embed              â”‚ Retrieve
       â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LLM Server   â”‚          â”‚ Milvus Vector DB â”‚
â”‚ (llama.cpp)  â”‚          â”‚ - Document chunks â”‚
â”‚ Mistral-7B   â”‚          â”‚ - Embeddings      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Component | Technology |
|-----------|-----------|
| **LLM** | Mistral-7B-Instruct-v0.2 (Q5_K_M quantization) |
| **Inference** | llama.cpp (OpenAI-compatible API) |
| **Embeddings** | sentence-transformers/all-MiniLM-L6-v2 |
| **Vector DB** | Milvus 2.3.10 (Docker) |
| **Backend** | FastAPI + LlamaIndex + Python 3.10+ |
| **Frontend** | SvelteKit + TypeScript |
| **GPU** | CUDA 12.8.1 (RTX A5000, 24GB VRAM) |

## ğŸ“ Project Structure

```
AIMentorProject/
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ chat_router.py  # Chat endpoints (REST + WebSocket)
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â””â”€â”€ config.py       # Configuration management
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â””â”€â”€ rag_service.py  # RAG orchestration logic
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry
â”‚   â”œâ”€â”€ ingest.py               # PDF ingestion CLI
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ .env.example            # Environment variables template
â”œâ”€â”€ frontend/                   # SvelteKit frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â”œâ”€â”€ components/     # Svelte components
â”‚   â”‚   â”‚   â”œâ”€â”€ stores.ts       # State management
â”‚   â”‚   â”‚   â””â”€â”€ api.ts          # Backend API client
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ +page.svelte    # Main chat page
â”‚   â””â”€â”€ package.json            # Node dependencies
â”œâ”€â”€ course_materials/           # Upload PDFs here
â”œâ”€â”€ docker-compose.yml          # Milvus infrastructure
â”œâ”€â”€ start_llm_server.sh         # LLM server startup script
â”œâ”€â”€ START_SERVICES.sh           # Automated setup script
â””â”€â”€ SETUP_STATUS.md             # Detailed setup guide
```

## ğŸ”§ Detailed Setup

### 1. Environment Setup

```bash
# Install system dependencies (if needed)
sudo apt-get update && sudo apt-get install -y \
  python3-venv python3-pip \
  nodejs npm \
  docker.io docker-compose \
  git curl wget

# Start Docker daemon
sudo systemctl start docker
```

### 2. Clone and Configure

```bash
git clone https://github.com/mrizvi96/AIMentorProject.git
cd AIMentorProject

# Create .env file (optional, defaults work)
cp backend/.env.example backend/.env
```

### 3. Download Model

```bash
# Create models directory
mkdir -p /workspace/models

# Download Mistral-7B (4.8GB, ~5-10 minutes)
wget -O /workspace/models/Mistral-7B-Instruct-v0.2.Q5_K_M.gguf \
  "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf"

# Verify download
ls -lh /workspace/models/
# Should show ~4.8GB file
```

### 4. Start Services

**Option A: Automated (Recommended)**
```bash
./START_SERVICES.sh
```

**Option B: Manual**
```bash
# Start Milvus
docker-compose up -d

# Setup Python environment
cd backend
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cd ..

# Install frontend dependencies
cd frontend
npm install
cd ..
```

### 5. Upload Course Materials

Upload your scholarly PDFs to `course_materials/`:

```bash
# Via SCP from local machine
scp /path/to/textbook.pdf root@RUNPOD_IP:/root/AIMentorProject/course_materials/

# Or using VS Code Remote-SSH:
# Right-click on course_materials/ â†’ Upload
```

### 6. Ingest Documents

```bash
cd backend
source venv/bin/activate
python ingest.py --directory ../course_materials

# Optional: Overwrite existing collection
python ingest.py --directory ../course_materials --overwrite
```

### 7. Start Servers

**Terminal 1 - LLM Server:**
```bash
./start_llm_server.sh
```

**Terminal 2 - Backend API:**
```bash
cd backend
source venv/bin/activate
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**Terminal 3 - Frontend:**
```bash
cd frontend
npm run dev -- --host 0.0.0.0
```

## ğŸ§ª Testing

### Health Checks

```bash
# Check LLM server
curl http://localhost:8080/v1/models

# Check backend
curl http://localhost:8000/

# Check Milvus
docker-compose ps
```

### Test Query

```bash
# Via API
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What is recursion?", "conversation_id": "test"}'

# Via UI
# Open http://localhost:5173 and ask a question
```

## ğŸ“Š API Endpoints

### REST API

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/` | Health check |
| GET | `/api/health` | Detailed service status |
| POST | `/api/chat` | Send chat message (non-streaming) |
| GET | `/api/chat/stats` | RAG service statistics |
| WS | `/api/ws/chat/{id}` | WebSocket streaming (Phase 2) |

### API Documentation

Interactive API docs available at:
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## ğŸ¯ Configuration

Edit `backend/.env` to customize:

```bash
# LLM Configuration
LLM_BASE_URL=http://localhost:8080/v1
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=512

# Milvus Configuration
MILVUS_HOST=localhost
MILVUS_PORT=19530
MILVUS_COLLECTION_NAME=course_materials

# RAG Configuration
CHUNK_SIZE=512
CHUNK_OVERLAP=50
TOP_K_RETRIEVAL=3
SIMILARITY_THRESHOLD=0.7
```

## ğŸ› Troubleshooting

### Model not found
```bash
ls -lh /workspace/models/
# Should show Mistral-7B-Instruct-v0.2.Q5_K_M.gguf (~4.8GB)
```

### Milvus connection failed
```bash
docker-compose ps
# All services should show "healthy"

docker-compose logs milvus
# Check for errors
```

### LLM server won't start
```bash
# Reinstall with CUDA support
pip uninstall llama-cpp-python
CMAKE_ARGS="-DGGML_CUDA=on" pip install --no-cache-dir "llama-cpp-python[server]"
```

### Out of GPU memory
```bash
# Check GPU usage
nvidia-smi

# Reduce context window in .env
LLM_MAX_TOKENS=256
```

### CORS errors in browser
- Verify backend is running on port 8000
- Check `main.py` CORS configuration
- Clear browser cache

## ğŸ“ˆ Development Status

**Phase 1 (Weeks 1-2): Simple RAG MVP** - âœ… 75% Complete

- âœ… Backend API with RAG service
- âœ… Frontend chat interface
- âœ… Milvus vector database integration
- âœ… PDF ingestion pipeline
- âœ… LLM inference setup
- â³ End-to-end testing (pending course materials)

**Phase 2 (Weeks 3-4): Agentic RAG** - ğŸ”œ Next

- LangGraph workflow implementation
- Self-correcting retrieve â†’ grade â†’ rewrite â†’ generate loop
- WebSocket streaming for real-time responses
- Query rewriting for better retrieval

**Phase 3 (Weeks 5-6): Production** - ğŸ“… Planned

- Systematic evaluation (20-question test bank)
- Prompt engineering refinement
- Full containerization (Docker)
- Operational runbook and monitoring

## ğŸ¤ Contributing

This is a capstone project. For questions or suggestions:
- Open an issue on GitHub
- Contact: mohammad.rizvi@csuglobal.edu

## ğŸ“„ License

Educational project - CSU Global Capstone

## ğŸ™ Acknowledgments

- **LlamaIndex** - RAG orchestration framework
- **Milvus** - Vector database
- **llama.cpp** - Efficient LLM inference
- **Mistral AI** - Base language model
- **SvelteKit** - Frontend framework

---

**Generated with Claude Code** - AI-assisted development for educational AI systems
