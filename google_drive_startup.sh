#!/bin/bash
# google_drive_startup.sh - Optimized AI Mentor startup using Google Drive cache
# Reduces startup time from 15-20 minutes to 3-5 minutes

set -e  # Exit on error

# Color codes
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo "================================================"
echo -e "${BLUE}üöÄ AI Mentor Google Drive Optimized Startup${NC}"
echo "================================================"
echo ""

# ============================================
# Configuration
# ============================================

# Google Drive File IDs (replace with your actual file IDs)
GOOGLE_DRIVE_MODEL_ID="${GOOGLE_DRIVE_MODEL_ID:-YOUR_MODEL_ID_HERE}"
GOOGLE_DRIVE_CHROMADB_ID="${GOOGLE_DRIVE_CHROMADB_ID:-YOUR_CHROMADB_ID_HERE}"
GOOGLE_DRIVE_MATERIALS_ID="${GOOGLE_DRIVE_MATERIALS_ID:-YOUR_MATERIALS_ID_HERE}"

# Cache control flags
USE_MODEL_CACHE="${USE_MODEL_CACHE:-true}"
USE_CHROMADB_CACHE="${USE_CHROMADB_CACHE:-true}"
USE_MATERIALS_CACHE="${USE_MATERIALS_CACHE:-true}"

# Local paths
MODEL_DIR="/workspace/models"
MODEL_FILE="$MODEL_DIR/mistral-7b-instruct-v0.2.Q5_K_M.gguf"
CHROMADB_DIR="./backend/chroma_db"
PROJECT_DIR="/workspace/AIMentorProject"

# ============================================
# Utility Functions
# ============================================

check_gdown() {
    if ! command -v gdown &> /dev/null; then
        echo -e "${YELLOW}Installing gdown for Google Drive downloads...${NC}"
        pip install -q gdown
    fi
}

download_from_gdrive() {
    local file_id=$1
    local output_path=$2
    local description=$3
    
    echo -e "${BLUE}Downloading $description...${NC}"
    
    if gdown "$file_id" -O "$output_path"; then
        echo -e "${GREEN}‚úì Downloaded: $description${NC}"
        ls -lh "$output_path"
    else
        echo -e "${RED}‚úó Failed to download: $description${NC}"
        return 1
    fi
    echo ""
}

verify_file() {
    local file_path=$1
    local description=$2
    local expected_size=$3
    
    if [ -f "$file_path" ]; then
        local actual_size=$(du -h "$file_path" | cut -f1)
        echo -e "${GREEN}‚úì $description verified ($actual_size)${NC}"
        return 0
    else
        echo -e "${RED}‚úó $description not found${NC}"
        return 1
    fi
}

# ============================================
# Pre-flight Checks
# ============================================

echo -e "${YELLOW}Pre-flight checks...${NC}"
echo ""

# Check if gdown is available
check_gdown

# Verify configuration
if [ "$GOOGLE_DRIVE_MODEL_ID" = "YOUR_MODEL_ID_HERE" ]; then
    echo -e "${RED}‚ö†Ô∏è  WARNING: Google Drive file IDs not configured${NC}"
    echo "Please set environment variables:"
    echo "  GOOGLE_DRIVE_MODEL_ID"
    echo "  GOOGLE_DRIVE_CHROMADB_ID (optional)"
    echo "  GOOGLE_DRIVE_MATERIALS_ID (optional)"
    echo ""
    echo "Example:"
    echo "  export GOOGLE_DRIVE_MODEL_ID=\"1AbC123xYz456\""
    echo "  ./google_drive_startup.sh"
    echo ""
fi

# Get Runpod IP
RUNPOD_IP=$(hostname -I | awk '{print $1}')
echo -e "${BLUE}üìç Pod IP: $RUNPOD_IP${NC}"
echo ""

# ============================================
# Step 1: Model Download/Verification
# ============================================

echo -e "${YELLOW}Step 1: Mistral Model Setup${NC}"
echo ""

mkdir -p "$MODEL_DIR"

if [ "$USE_MODEL_CACHE" = true ] && [ "$GOOGLE_DRIVE_MODEL_ID" != "YOUR_MODEL_ID_HERE" ]; then
    # Try to download from Google Drive
    if download_from_gdrive "$GOOGLE_DRIVE_MODEL_ID" "$MODEL_FILE" "Mistral Model from Google Drive"; then
        echo -e "${GREEN}‚úì Model downloaded from Google Drive cache${NC}"
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Google Drive download failed, falling back to HuggingFace${NC}"
        USE_MODEL_CACHE=false
    fi
fi

if [ "$USE_MODEL_CACHE" = false ] || ! verify_file "$MODEL_FILE" "Model file" "4.8GB"; then
    echo -e "${YELLOW}Downloading model from HuggingFace (fallback)...${NC}"
    download_from_gdrive "TheBloke/Mistral-7B-Instruct-v0.2-GGUF" "$MODEL_FILE" "Mistral Model from HuggingFace"
    
    # Update wget URL for HuggingFace
    wget --progress=bar:force:noscroll "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q5_K_M.gguf" -O "$MODEL_FILE"
fi

echo ""

# ============================================
# Step 2: Course Materials Setup
# ============================================

echo -e "${YELLOW}Step 2: Course Materials Setup${NC}"
echo ""

cd "$PROJECT_DIR"

# Create course_materials directory
mkdir -p course_materials

if [ "$USE_MATERIALS_CACHE" = true ] && [ "$GOOGLE_DRIVE_MATERIALS_ID" != "YOUR_MATERIALS_ID_HERE" ]; then
    echo -e "${BLUE}Attempting to download course materials from Google Drive...${NC}"
    
    # Try to download as ZIP file first
    if download_from_gdrive "$GOOGLE_DRIVE_MATERIALS_ID" "course_materials.zip" "Course Materials ZIP"; then
        echo -e "${BLUE}Extracting course materials...${NC}"
        if unzip -q course_materials.zip -d course_materials/; then
            echo -e "${GREEN}‚úì Course materials extracted${NC}"
            rm -f course_materials.zip
        else
            echo -e "${YELLOW}‚ö†Ô∏è  ZIP extraction failed, will use individual downloads${NC}"
            rm -f course_materials.zip
            USE_MATERIALS_CACHE=false
        fi
    else
        echo -e "${YELLOW}‚ö†Ô∏è  Google Drive download failed, using existing script${NC}"
        USE_MATERIALS_CACHE=false
    fi
fi

# Fallback to original download script
if [ "$USE_MATERIALS_CACHE" = false ]; then
    echo -e "${BLUE}Using individual PDF downloads...${NC}"
    if [ -f "download_textbooks.sh" ]; then
        chmod +x download_textbooks.sh
        ./download_textbooks.sh
    else
        echo -e "${RED}Error: download_textbooks.sh not found${NC}"
        exit 1
    fi
fi

# Verify course materials
pdf_count=$(ls -1 course_materials/*.pdf 2>/dev/null | wc -l)
echo -e "${GREEN}‚úì Course materials ready: $pdf_count PDF files${NC}"
echo ""

# ============================================
# Step 3: ChromaDB Setup (Optional Cache)
# ============================================

echo -e "${YELLOW}Step 3: Vector Database Setup${NC}"
echo ""

if [ "$USE_CHROMADB_CACHE" = true ] && [ "$GOOGLE_DRIVE_CHROMADB_ID" != "YOUR_CHROMADB_ID_HERE" ]; then
    echo -e "${BLUE}Attempting to restore ChromaDB from Google Drive...${NC}"
    
    if download_from_gdrive "$GOOGLE_DRIVE_CHROMADB_ID" "chroma_db_backup.tar.gz" "ChromaDB Backup"; then
        echo -e "${BLUE}Restoring ChromaDB...${NC}"
        if tar -xzf chroma_db_backup.tar.gz -C backend/; then
            echo -e "${GREEN}‚úì ChromaDB restored from cache${NC}"
            rm -f chroma_db_backup.tar.gz
            
            # Verify the database
            if [ -d "$CHROMADB_DIR" ]; then
                chromadb_size=$(du -sh "$CHROMADB_DIR" | cut -f1)
                echo -e "${GREEN}‚úì ChromaDB verified ($chromadb_size)${NC}"
            fi
        else
            echo -e "${YELLOW}‚ö†Ô∏è  ChromaDB restore failed, will re-ingest${NC}"
            rm -f chroma_db_backup.tar.gz
            USE_CHROMADB_CACHE=false
        fi
    else
        echo -e "${YELLOW}‚ö†Ô∏è  ChromaDB download failed, will re-ingest${NC}"
        USE_CHROMADB_CACHE=false
    fi
fi

# Fallback: Set up for ingestion
if [ "$USE_CHROMADB_CACHE" = false ] || ! [ -d "$CHROMADB_DIR" ]; then
    echo -e "${BLUE}ChromaDB cache not available, setup for ingestion${NC}"
    echo "Note: Run ingestion after startup with:"
    echo "  cd backend && source venv/bin/activate"
    echo "  python ingest.py --directory ../course_materials/"
fi

echo ""

# ============================================
# Step 4: Backend Environment Setup
# ============================================

echo -e "${YELLOW}Step 4: Backend Environment Setup${NC}"
echo ""

cd backend

# Create virtual environment if it doesn't exist
if [ ! -d "venv" ]; then
    echo -e "${BLUE}Creating Python virtual environment...${NC}"
    python3 -m venv venv
fi

# Activate and install dependencies
source venv/bin/activate

echo -e "${BLUE}Installing Python dependencies...${NC}"
pip install -q --upgrade pip setuptools wheel

if [ -f "requirements.txt" ]; then
    pip install -q -r requirements.txt
else
    echo -e "${RED}Error: requirements.txt not found${NC}"
    exit 1
fi

# Install llama-cpp-python with CUDA support
echo -e "${BLUE}Installing llama-cpp-python with CUDA support...${NC}"
CMAKE_ARGS="-DGGML_CUDA=on" FORCE_CMAKE=1 pip install --no-cache-dir "llama-cpp-python[server]"

# Fix numpy version if needed
echo -e "${BLUE}Fixing numpy version compatibility...${NC}"
pip install "numpy<2.0.0" --force-reinstall

echo -e "${GREEN}‚úì Backend environment ready${NC}"
echo ""

# ============================================
# Step 5: Start Services
# ============================================

echo -e "${YELLOW}Step 5: Starting Services${NC}"
echo ""

# Start LLM Server
echo -e "${BLUE}Starting LLM Inference Server...${NC}"

# Kill existing session if it exists
tmux kill-session -t llm 2>/dev/null || true

# Start LLM server in tmux
tmux new-session -d -s llm "cd '$PROJECT_DIR' && source backend/venv/bin/activate && python3 -m llama_cpp.server \
    --model '$MODEL_FILE' \
    --n_gpu_layers -1 \
    --n_ctx 4096 \
    --host 0.0.0.0 \
    --port 8080 \
    --chat_format mistral-instruct"

echo -e "${GREEN}‚úì LLM server started in tmux session 'llm'${NC}"
echo "  View logs: tmux attach -t llm"
echo ""

# Wait for model to load
echo -e "${BLUE}Waiting 30 seconds for model to load into GPU...${NC}"
sleep 30

# Start Backend API
echo -e "${BLUE}Starting Backend API Server...${NC}"

# Kill existing backend session
tmux kill-session -t backend 2>/dev/null || true

# Start backend in tmux
tmux new-session -d -s backend "cd '$PROJECT_DIR/backend' && source venv/bin/activate && uvicorn main:app --host 0.0.0.0 --port 8000 --reload"

echo -e "${GREEN}‚úì Backend API started in tmux session 'backend'${NC}"
echo "  View logs: tmux attach -t backend"
echo ""

# ============================================
# Step 6: Health Checks
# ============================================

echo -e "${YELLOW}Step 6: Health Checks${NC}"
echo ""

sleep 5

# Check LLM Server
echo -n "  ‚Ä¢ LLM Server:   "
if curl -s http://localhost:8080/v1/models > /dev/null 2>&1; then
    MODEL_NAME=$(curl -s http://localhost:8080/v1/models | jq -r '.data[0].id' 2>/dev/null || echo "unknown")
    echo -e "${GREEN}‚úì Running (model: $MODEL_NAME)${NC}"
else
    echo -e "${YELLOW}‚è≥ Still loading (check: tmux attach -t llm)${NC}"
fi

# Check Backend API
echo -n "  ‚Ä¢ Backend API:  "
if curl -s http://localhost:8000/ > /dev/null 2>&1; then
    echo -e "${GREEN}‚úì Running on port 8000${NC}"
else
    echo -e "${YELLOW}‚è≥ Still starting (check: tmux attach -t backend)${NC}"
fi

# Check GPU
echo -n "  ‚Ä¢ GPU:          "
if command -v nvidia-smi &> /dev/null; then
    GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader 2>/dev/null | head -1)
    GPU_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits 2>/dev/null | head -1)
    echo -e "${GREEN}‚úì $GPU_NAME (${GPU_MEM} MB used)${NC}"
else
    echo -e "${RED}‚úó nvidia-smi not found${NC}"
fi

echo ""
echo "================================================"
echo -e "${GREEN}‚úÖ Google Drive Optimized Startup Complete!${NC}"
echo "================================================"
echo ""
echo "üìå Service URLs (Public - if ports exposed):"
echo "   Replace '[POD-ID]' with your actual Runpod pod ID"
echo "   ‚Ä¢ Backend API:  https://[POD-ID]-8000.runpod.io"
echo "   ‚Ä¢ API Docs:     https://[POD-ID]-8000.runpod.io/docs"
echo "   ‚Ä¢ LLM Server:   https://[POD-ID]-8080.runpod.io"
echo ""
echo "üìå Service URLs (Internal):"
echo "   ‚Ä¢ Backend API:  http://localhost:8000"
echo "   ‚Ä¢ LLM Server:   http://localhost:8080"
echo ""
echo "üîß Useful Commands:"
echo "   ‚Ä¢ View LLM logs:       tmux attach -t llm"
echo "   ‚Ä¢ View API logs:       tmux attach -t backend"
echo "   ‚Ä¢ Detach from tmux:     Ctrl+B then D"
echo "   ‚Ä¢ Check GPU usage:      nvidia-smi"
echo "   ‚Ä¢ Stop LLM:            tmux kill-session -t llm"
echo "   ‚Ä¢ Stop Backend:        tmux kill-session -t backend"
echo "   ‚Ä¢ Stop all:            tmux kill-session -t llm && tmux kill-session -t backend"
echo ""
echo "üìö Cache Status:"
if [ "$USE_MODEL_CACHE" = true ]; then
    echo "   ‚Ä¢ Model Cache:         ‚úÖ Used"
else
    echo "   ‚Ä¢ Model Cache:         ‚ùå Fallback to HuggingFace"
fi

if [ "$USE_CHROMADB_CACHE" = true ] && [ -d "$CHROMADB_DIR" ]; then
    echo "   ‚Ä¢ ChromaDB Cache:      ‚úÖ Used"
elif [ "$USE_CHROMADB_CACHE" = true ]; then
    echo "   ‚Ä¢ ChromaDB Cache:      ‚ö†Ô∏è  Attempted but failed"
else
    echo "   ‚Ä¢ ChromaDB Cache:      ‚ùå Not used"
fi

if [ "$USE_MATERIALS_CACHE" = true ]; then
    echo "   ‚Ä¢ Materials Cache:      ‚úÖ Used"
else
    echo "   ‚Ä¢ Materials Cache:      ‚ùå Fallback to individual downloads"
fi

echo ""
echo "üìö Next Steps:"
if ! [ -d "$CHROMADB_DIR" ]; then
    echo "   ${YELLOW}1. Ingest course materials (ChromaDB cache not available):${NC}"
    echo "      cd backend && source venv/bin/activate"
    echo "      python ingest.py --directory ../course_materials/"
    echo ""
fi

echo "   2. Test the system:"
echo "      curl -X POST http://localhost:8000/api/chat \\"
echo "        -H 'Content-Type: application/json' \\"
echo "        -d '{\"message\": \"What is Python?\", \"conversation_id\": \"test\"}' | jq"
echo ""
echo "   3. Setup frontend (on your local machine):"
echo "      cd frontend"
echo "      # Edit frontend/src/lib/api.ts:"
echo "      # Change API_BASE_URL to: 'https://[POD-ID]-8000.runpod.io'"
echo "      npm install"
echo "      npm run dev"
echo ""
echo "   4. Backup to Google Drive (optional):"
echo "      # After making changes to ChromaDB:"
echo "      tar -czf chroma_backup-\$(date +%Y%m%d).tar.gz backend/chroma_db/"
echo "      # Upload to Google Drive for next time"
echo ""
echo "================================================"
echo -e "${GREEN}Startup time with Google Drive cache: ~3-5 minutes${NC}"
echo "Traditional startup time: 15-20 minutes"
echo -e "${GREEN}Time saved: 10-17 minutes (75-85% faster!)${NC}"
echo "================================================"